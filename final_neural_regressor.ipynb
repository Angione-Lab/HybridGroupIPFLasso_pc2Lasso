{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import copy                                                # to keep track of the evolution of the weights of the network\n",
    "import os   \n",
    "# from sklearn import manifold                               # for the t-SNE\n",
    "# import matplotlib                                          # for the t-SNE\n",
    "# from itertools import cycle, islice                        # for the t-SNE\n",
    "from matplotlib import pyplot as plt \n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import torch                       \n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import transforms\n",
    "from datasets import YeastDataset # dataloader loading the yeast dataset in the correct format, replace it with one suitable for your problem\n",
    "from data_elaboration_utilities import *\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the parameters and define the datasets\n",
    "records_path = \"\"\n",
    "\n",
    "records = pd.read_csv(records_path, encoding=\"utf-8\")   \n",
    "best = sorted(records[\"validation_loss\"])[:10]               \n",
    "parameters = records.loc[records[\"validation_loss\"] == best[i]].iloc[0]\n",
    "\n",
    "no_cuda = False               # SHOULD BE FALSE                    \n",
    "seed = 1                   \n",
    "log_interval = 10       \n",
    "percent_train = 0.7  \n",
    "percent_validation = 0.2\n",
    "\n",
    "dataset_paths = params[\"dataset_path\"]\n",
    "parameters[\"dataset_path\"] = dataset_path\n",
    "\n",
    "training_path = paths[-1]                                                      \n",
    "test_path = paths[-1] \n",
    "parameters[\"training_path\"] = training_path\n",
    "parameters[\"test_path\"] = test_path\n",
    "                                               \n",
    "input_size = 19\n",
    "\n",
    "reconstruction_weight = [input_size]          # default\n",
    "\n",
    "num_neurons = params[\"num_neurons\"]\n",
    "\n",
    "z_size = params[\"z_size\"]\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "partition, labels = splitTrainingValidationTestSet(dataset_path, percent_train, percent_validation)\n",
    "    \n",
    "training_set = YeastDataset(partition['training'], labels)\n",
    "validation_set = YeastDataset(partition['validation'], labels)\n",
    "test_set = YeastDataset(partition['test'], labels)\n",
    "        \n",
    "torch.set_printoptions(precision=9)                                 # to print more digits for the loss\n",
    "\n",
    "print(\"Device: \", device)                           # simple check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trialRegressor(nn.Module):\n",
    "    def __init__(self, num_neurons, num_neurons2, dropout):  \n",
    "        super(trialRegressor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_neurons2 = num_neurons2\n",
    "        self.dropout = dropout\n",
    "            \n",
    "        self.fc1 = nn.Linear(self.input_size, self.num_neurons)\n",
    "        self.fc2 = nn.Linear(self.num_neurons, self.num_neurons2)\n",
    "        self.fc3 = nn.Linear(self.num_neurons2, 1)\n",
    "        self.dropt = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x.float()))\n",
    "        h1 = F.relu(self.fc2(self.dropt(h)))\n",
    "        return self.fc3(self.dropt(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, predict, loss):\n",
    "    if loss == \"L1_loss\":\n",
    "        return F.l1_loss(predict, real)\n",
    "    elif loss == \"MSE_loss\":\n",
    "        return F.mse_loss(predict, real)\n",
    "    else:\n",
    "        return F.smooth_l1_loss(predict, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_function(optimizer, model, lr):\n",
    "    if optimizer == \"Adam\":\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"SGD\":\n",
    "        return optim.SGD(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        return optim.Adadelta(model.parameters(), lr=lr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, lo):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for _, (data, labels) in enumerate(train_loader):\n",
    "        # print(\"Epoch: {}, Batch index: {}\".format(epoch, batch_idx))\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optmz.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(labels.float(), output, lo)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optmz.step()\n",
    "    with torch.no_grad():\n",
    "        loss_evolution.append(loss.cpu().detach().numpy())   \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(          \n",
    "        epoch, loss / len(train_loader.dataset)))\n",
    "\n",
    "    # print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "    #      epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch, lo):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        batch_stop = (len(test_loader.dataset) // parameters[\"batch_size\"]) \\\n",
    "        if (len(test_loader.dataset) % parameters[\"batch_size\"]) else \\\n",
    "        (len(test_loader.dataset) // parameters[\"batch_size\"] -1)\n",
    "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_function(labels.float(), output, lo)\n",
    "            test_loss += loss.item()\n",
    "            if epoch == parameters[\"epochs\"] and batch_idx == batch_stop:\n",
    "                REC_targets.append(labels.cpu().numpy().reshape(1, -1))\n",
    "                REC_predictions.append(output.cpu().reshape(1, -1))\n",
    "        test_loss_evolution.append(loss.cpu().numpy()) \n",
    "    # valid_loss /= len(validation_loader.dataset)                              \n",
    "    # print('====> Validation set loss: {:.4f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"path to the directory where to save the final results for the dataset\\TEST\" + \"\\\\\" + str(datetime.datetime.now().\n",
    "                                                                                     strftime(\"%d_%m_%Y-%H.%M\"))\n",
    "model = trialRegressor(parameters[\"num_neurons\"], parameters[\"num_neurons2\"], parameters[\"dropout\"]).to(device)\n",
    "       \n",
    "optmz = optimizer_function(parameters[\"optimizer\"], model, parameters[\"learning_rate\"])\n",
    "step_size = parameters[\"epochs\"]//2\n",
    "gamma = parameters[\"gamma\"]\n",
    "scheduler = optim.lr_scheduler.StepLR(optmz, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(training_set, shuffle=True, batch_size=parameters[\"batch_size\"], **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=parameters[\"batch_size\"], shuffle=True, **kwargs)\n",
    "    \n",
    "REC_targets = list()\n",
    "REC_predictions = list()\n",
    "loss_evolution = list()              # superfluous\n",
    "test_loss_evolution = list()\n",
    "    \n",
    "for epoch in range(1, parameters[\"epochs\"] + 1):                                   \n",
    "    scheduler.step()\n",
    "    train(epoch, parameters[\"loss\"])\n",
    "    test(epoch, parameters[\"loss\"])  \n",
    "        \n",
    "if not os.path.exists(directory_path):                \n",
    "    os.makedirs(directory_path)\n",
    "    \n",
    "# save the losses for comparison (superfluous given the saves in the csv file)\n",
    "text_file = open(\"where tp save all the losses \\Losses.txt\", \"a\")\n",
    "text_file.write(\"\\n\" + directory_path[68:] + \"  \" + str(test_loss_evolution[-10:]))  # change according to your directory_path\n",
    "text_file.close()\n",
    "    \n",
    "torch.save(model.state_dict(), directory_path + \"\\\\weights.pt\")   \n",
    "    \n",
    "visualizeLossesOverEpochs(test_loss_evolution, 0, 0, \"Test loss\", \"0\", \"0\", 50, 100, 'o-', False)\n",
    "\n",
    "plt.savefig(fname=directory_path + \"\\\\test_loss.png\", bbox_inches=\"tight\")\n",
    "    \n",
    "# save full test loss\n",
    "text_file = open(directory_path +\"\\\\test_loss.txt\", \"w\")\n",
    "text_file.write(str(test_loss_evolution) + \"\\n\")\n",
    "text_file.close()\n",
    "    \n",
    "# save last batch targets\n",
    "text_file = open(directory_path +\"\\\\last_batch_targets.txt\", \"w\")\n",
    "text_file.write(str(REC_targets) + \"\\n\")\n",
    "text_file.close()\n",
    "    \n",
    "# save last batch predictions\n",
    "text_file = open(directory_path +\"\\\\last_batch_predictions.txt\", \"w\")\n",
    "text_file.write(str(REC_predictions) + \"\\n\")\n",
    "text_file.close()\n",
    "    \n",
    "parameters[\"training_loss\"] = loss_evolution[-1]              \n",
    "parameters[\"test_loss_evolution\"] = test_loss_evolution[-1]\n",
    "parameters[\"training_path\"] = str(training_path)\n",
    "parameters[\"test_path\"] = str(test_path)\n",
    "parameters[\"experiment\"] = directory_path[68:]    \n",
    "\n",
    "text_file = open(directory_path + \"\\\\\" + \"Parameters.txt\", \"w\")\n",
    "text_file.write(\"Parameters used: \\n\\n\")\n",
    "for _, (key, value) in enumerate(parameters.items()):\n",
    "    text_file.write(key + \" = \" + str(value) + \"\\n\")\n",
    "text_file.write(\"\\n Dataset path: \" + dataset_path)\n",
    "text_file.write(\"\\n Training set path: \" + str(training_path))  \n",
    "text_file.write(\"\\n Test set path: \" + str(test_path))\n",
    "    \n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
