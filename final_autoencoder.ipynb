{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import copy                                                # to keep track of the evolutoin of the weights of the network\n",
    "import os   \n",
    "# from sklearn import manifold                               # for the t-SNE\n",
    "# import matplotlib                                          # for the t-SNE\n",
    "# from itertools import cycle, islice                        # for the t-SNE\n",
    "from matplotlib import pyplot as plt \n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import torch                        \n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import transforms\n",
    "from datasets import YeastDataset # dataloader loading the yeast dataset in the correct format, replace it with one suitable for your problem\n",
    "from data_elaboration_utilities import *\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the parameters and define the datasets\n",
    "records_path = \"where the previsously tried combinations are stored \\Records.csv\"\n",
    "experiment_path = \"specific experiment\"    \n",
    "records = pd.read_csv(records_path, encoding=\"utf-8\")   \n",
    "# best = sorted(records[\"validation_loss\"])[:10]               \n",
    "# parameters = records.loc[records[\"validation_loss\"] == best[0]].iloc[0]\n",
    "parameters = records.loc[records[\"experiment\"] == experiment_path].iloc[0]\n",
    "\n",
    "no_cuda = False               # SHOULD BE FALSE                    \n",
    "seed = 1                   \n",
    "log_interval = 10       \n",
    "percent_train = 0.7  \n",
    "percent_validation = 0.2\n",
    "\n",
    "dataset_path = parameters[\"dataset_path\"]\n",
    "\n",
    "training_path = \"\"                                                    \n",
    "test_path = \"\" \n",
    "parameters[\"training_path\"] = training_path\n",
    "parameters[\"test_path\"] = test_path\n",
    "\n",
    "input_size = parameters[\"reconstruction_weight\"]  # default\n",
    "\n",
    "reconstruction_weight =   [input_size]        \n",
    "\n",
    "num_neurons = parameters[\"num_neurons\"]\n",
    "\n",
    "z_size = parameters[\"z_size\"]\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")               # use GPU if available\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "partition, labels = splitTrainingValidationTestSet(dataset_path, percent_train, percent_validation)\n",
    "    \n",
    "training_set = YeastDataset(partition['training'], labels)\n",
    "validation_set = YeastDataset(partition['validation'], labels)\n",
    "test_set = YeastDataset(partition['test'], labels)\n",
    "        \n",
    "torch.set_printoptions(precision=9)                                 # to print more digits for the loss\n",
    "\n",
    "print(\"Device: \", device)                           # simple check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trialVAE(nn.Module):\n",
    "    def __init__(self, num_neurons, z_size, dropout):\n",
    "        super(trialVAE, self).__init__()\n",
    "    \n",
    "        self.input_size = input_size\n",
    "        self.num_neurons = num_neurons\n",
    "        self.z_size = z_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, self.num_neurons)\n",
    "        self.fc21 = nn.Linear(self.num_neurons, self.z_size)\n",
    "        self.fc22 = nn.Linear(self.num_neurons, self.z_size)\n",
    "        self.fc3 = nn.Linear(self.z_size, self.num_neurons)\n",
    "        self.fc4 = nn.Linear(self.num_neurons, self.input_size)\n",
    "        self.dropt = nn.Dropout(self.dropout)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x.float()))               \n",
    "        # h1 = torch.tanh(self.fc1(x.float()))      # MSE doesn't learn and all the weights change at approximately each epoch\n",
    "        return self.fc21(self.dropt(h1)), self.fc22(self.dropt(h1))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        # h3 = torch.tanh(self.fc3(z))              # MSE doesn't learn and all the weights change at approximately each epoch\n",
    "        return torch.sigmoid(self.fc4(self.dropt(h3)))         \n",
    "        # return F.softmax(self.fc4(h3), dim=1)     # MSE doesn't decrease with it\n",
    "        # return torch.tanh(self.fc4(h3))           # MSE doesn't decrease like with the sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)        # representation to get\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(parameters[\"batch_size\"])\n",
    "model = trialVAE(parameters[\"num_neurons\"], parameters[\"z_size\"], parameters[\"dropout\"])\n",
    "model.load_state_dict(torch.load(\"path to the weights of the selected experiment\" + \"\\\\\" + parameters[\"experiment\"] + \"\\\\\" + \"weights.pt\"), \n",
    "                                         strict=True)\n",
    "model.to(device)\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(training_set, shuffle=True, batch_size=batch_size, **kwargs)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "processed_datasets = [(train_loader, \"training\", # modify the path below so that it is possible to get more processed datasets\n",
    "                       # for the same original dataset\n",
    "                       \"training-associated file.csv\"), \n",
    "                      (validation_loader, \"validation\", \"validation-associated file.csv\"), \n",
    "                      (test_loader, \"test\", \"test-associated file.csv\")\n",
    "                     ]  \n",
    "\n",
    "directory_path = \"where to save the results\" + \"\\\\\"\n",
    "\n",
    "for processed in processed_datasets:\n",
    "    new_data = list()\n",
    "    for i in range(len(processed[0].dataset)):             # len(partition[\"training\"])\n",
    "        # data, label = training_set.__getitem__(i)        # equivalent to using dataloader except for the fact that \n",
    "        data = partition[processed[1]][i][2:]                # you're not using tensors\n",
    "        label = partition[processed[1]][i][0]           \n",
    "        value = partition[processed[1]][i][1]\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            data = torch.from_numpy(data)                  # now IT IS EQUIVALENT to using dataloader\n",
    "            data = data.to(device)\n",
    "            mu, logvar = model.encode(data)               # process the data with the autoencoder\n",
    "            z = model.reparameterize(mu, logvar).cpu()\n",
    "            z = z.numpy()\n",
    "            element = [label]\n",
    "            value = [value]\n",
    "            element.extend(value)\n",
    "            element.extend(z)\n",
    "            new_data.append(element)  \n",
    "        \n",
    "    new_dataset = pd.DataFrame(np.array(new_data))                     \n",
    "    new_dataset.rename(columns={0: 'Row', 1: 'log2relT'}, inplace=True)\n",
    "    new_dataset.to_csv(directory_path + processed[2], encoding=\"utf-8\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
