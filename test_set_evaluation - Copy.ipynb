{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import copy                                                # to keep track of the evolution of the weights of the network\n",
    "import os   \n",
    "# from sklearn import manifold                               # for the t-SNE\n",
    "# import matplotlib                                          # for the t-SNE\n",
    "# from itertools import cycle, islice                        # for the t-SNE\n",
    "from matplotlib import pyplot as plt \n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import torch                        \n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import transforms\n",
    "from datasets import YeastDataset # dataloader loading the yeast dataset in the correct format, replace it with one suitable for your problem\n",
    "from data_elaboration_utilities import *\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover the parameters and define the datasets\n",
    "starting_time = time.time()\n",
    "base_path = \"path of the directory of interest\"\n",
    "records_path = base_path + \"\\Records.csv\"\n",
    "\n",
    "records = pd.read_csv(records_path, encoding=\"utf-8\")   \n",
    "best = sorted(records[\"validation_loss\"])[:10]               \n",
    "parameters = records.loc[records[\"validation_loss\"] == best[0]].iloc[0]  # we take only the first element, even if it doesn't  \n",
    "                                                                         # mean it's the best (better the best mean)\n",
    "                                                                        \n",
    "\n",
    "no_cuda = False               # SHOULD BE FALSE                    \n",
    "seed = 1                   \n",
    "log_interval = 10       \n",
    "percent_train = 0.7  \n",
    "percent_validation = 0.2\n",
    "\n",
    "dataset_path = parameters[\"dataset_path\"]         \n",
    "parameters[\"dataset_path\"] = dataset_path\n",
    "\n",
    "training_path = parameters[\"training_path\"]                                                     \n",
    "test_path = parameters[\"test_path\"]               \n",
    "parameters[\"training_path\"] = training_path\n",
    "parameters[\"test_path\"] = test_path\n",
    "                                            \n",
    "input_size = \n",
    "\n",
    "reconstruction_weight = [input_size]          # default\n",
    "\n",
    "num_neurons = parameters[\"num_neurons\"]\n",
    "\n",
    "# z_size = parameters[\"z_size\"]\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")   # use GPU if available\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# partition, labels = loadTrainingTestSet(training_path, test_path)\n",
    "partition, labels = splitTrainingValidationTestSet(dataset_path, percent_train, percent_validation)\n",
    "\n",
    "test_set = YeastDataset(partition['test'], labels)\n",
    "        \n",
    "torch.set_printoptions(precision=9)                                 # to print more digits for the loss\n",
    "\n",
    "print(\"Device: \", device)                           # simple check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trialRegressor(nn.Module):\n",
    "    def __init__(self, num_neurons, num_neurons2, dropout):  \n",
    "        super(trialRegressor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_neurons2 = num_neurons2\n",
    "        self.dropout = dropout\n",
    "            \n",
    "        self.fc1 = nn.Linear(self.input_size, self.num_neurons)\n",
    "        self.fc2 = nn.Linear(self.num_neurons, self.num_neurons2)\n",
    "        self.fc3 = nn.Linear(self.num_neurons2, 1)\n",
    "        self.dropt = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x.float()))\n",
    "        h1 = F.relu(self.fc2(self.dropt(h)))\n",
    "        return self.fc3(self.dropt(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, predict, loss):\n",
    "    if loss == \"L1_loss\":\n",
    "        return F.l1_loss(predict, real)\n",
    "    elif loss == \"MSE_loss\":\n",
    "        return F.mse_loss(predict, real)\n",
    "    else:\n",
    "        return F.smooth_l1_loss(predict, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_function(optimizer, model, lr):\n",
    "    if optimizer == \"Adam\":\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"SGD\":\n",
    "        return optim.SGD(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        return optim.Adadelta(model.parameters(), lr=lr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(lo):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_function(labels.float(), output, lo)\n",
    "            test_loss += loss.item()\n",
    "            REC_targets.append(labels.cpu().numpy().reshape(1, -1))\n",
    "            REC_predictions.append(output.cpu().reshape(1, -1))\n",
    "        test_loss_evolution.append(loss.cpu().numpy()) \n",
    "    # valid_loss /= len(validation_loader.dataset)                              \n",
    "    # print('====> Validation set loss: {:.4f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = base_path + \"\\TEST\"\n",
    "model = trialRegressor(parameters[\"num_neurons\"], parameters[\"num_neurons2\"], parameters[\"dropout\"]).to(device)\n",
    "     \n",
    "optmz = optimizer_function(parameters[\"optimizer\"], model, parameters[\"learning_rate\"])\n",
    "step_size = parameters[\"epochs\"]//2\n",
    "gamma = parameters[\"gamma\"]\n",
    "scheduler = optim.lr_scheduler.StepLR(optmz, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=int(parameters[\"batch_size\"]), shuffle=True, **kwargs)\n",
    "    \n",
    "REC_targets = list()\n",
    "REC_predictions = list()\n",
    "test_loss_evolution = list() \n",
    "\n",
    "model.load_state_dict(torch.load(base_path + parameters[\"experiment\"][-17:] + \"\\weights.pt\"))\n",
    "test(parameters[\"loss\"])  \n",
    "\n",
    "if not os.path.exists(directory_path):                \n",
    "    os.makedirs(directory_path)\n",
    "    \n",
    "# save the losses for comparison (superfluous given the saves in the csv file)\n",
    "text_file = open(directory_path + \"\\Losses.txt\", \"a\")\n",
    "text_file.write(\"\\n\" + directory_path[68:] + \"  \" + str(test_loss_evolution[-10:])) # change according to your directory_path\n",
    "text_file.close()  \n",
    "    \n",
    "visualizeLossesOverEpochs(test_loss_evolution, 0, 0, \"Test loss\", \"0\", \"0\", 50, 100, 'o-', False)\n",
    "\n",
    "plt.savefig(fname=directory_path + \"\\\\test_loss.png\", bbox_inches=\"tight\")\n",
    "    \n",
    "# save full test loss\n",
    "text_file = open(directory_path +\"\\\\test_loss.txt\", \"w\")\n",
    "text_file.write(str(test_loss_evolution) + \"\\n\")\n",
    "text_file.close()\n",
    "    \n",
    "# save last batch targets\n",
    "text_file = open(directory_path +\"\\\\last_batch_targets.txt\", \"w\")\n",
    "text_file.write(str(REC_targets) + \"\\n\")\n",
    "text_file.close()\n",
    "    \n",
    "# save last batch predictions\n",
    "text_file = open(directory_path +\"\\\\last_batch_predictions.txt\", \"w\")\n",
    "text_file.write(str(REC_predictions) + \"\\n\")\n",
    "text_file.close()\n",
    "                 \n",
    "parameters[\"test_loss_evolution\"] = test_loss_evolution[-1]\n",
    "parameters[\"training_path\"] = str(training_path)\n",
    "parameters[\"test_path\"] = str(test_path)\n",
    "parameters[\"experiment\"] = directory_path[68:]    \n",
    "\n",
    "text_file = open(directory_path + \"\\\\\" + \"Parameters.txt\", \"w\")\n",
    "text_file.write(\"Parameters used: \\n\\n\")\n",
    "for _, (key, value) in enumerate(parameters.items()):\n",
    "    text_file.write(key + \" = \" + str(value) + \"\\n\")\n",
    "# text_file.write(\"\\n Dataset path: \" + dataset_path)\n",
    "text_file.write(\"\\n Training set path: \" + str(training_path))  \n",
    "text_file.write(\"\\n Test set path: \" + str(test_path))\n",
    "    \n",
    "text_file.close()\n",
    "print(\"Time required for testing: {} minutes\".format((time.time() - starting_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the losses for comparison whith the other methods\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from warnings import filterwarnings\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "def readLoss(path):                       \n",
    "    previous_loss = open(path, \"r\")\n",
    "    prev = previous_loss.read()\n",
    "    prev = prev.replace(\"array\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"dtype=float32\", \"\") \\\n",
    "    .replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"\").replace(\"tensor\", \"\").replace(\"\\n\", \"\")\n",
    "    prev = prev.split(\",\")\n",
    "    compar = list()\n",
    "    for j in prev:\n",
    "        compar.append(j)\n",
    "    second = [float(x) for x in compar]                          # loss to compare to the current one\n",
    "    return second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = readLoss(directory_path + \"\\last_batch_targets.txt\")\n",
    "predictions = readLoss(directory_path + \"\\last_batch_predictions.txt\")\n",
    "\n",
    "print(\"MSE: {}, MAE: {}, R2 : {}\".format(mean_squared_error(target, predictions), \n",
    "                                         mean_absolute_error(target, predictions), r2_score(target, predictions)))\n",
    "\n",
    "print(\"Wilcoxon rank: \", wilcoxon(target, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
