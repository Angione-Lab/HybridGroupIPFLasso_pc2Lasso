{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import copy                                                # to keep track of the evolution of the weights of the network\n",
    "import os   \n",
    "# from sklearn import manifold                               # for the t-SNE\n",
    "# import matplotlib                                          # for the t-SNE\n",
    "# from itertools import cycle, islice                        # for the t-SNE\n",
    "from matplotlib import pyplot as plt \n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import torch                        \n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "# from torchvision import transforms\n",
    "from datasets import YeastDataset # dataloader loading the yeast dataset in the correct format, replace it with one suitable for your problem\n",
    "from data_elaboration_utilities import *\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the arrays, the dictionary and the variables\n",
    "repetitions = 50                                                                # number of random attempts\n",
    "\n",
    "parameters = {\"batch_size\" : None, \n",
    "              \"epochs\" : None,                          \n",
    "              \"learning_rate\" : None,\n",
    "              \"dataset_path\" : None,\n",
    "              \"training_path\" : None,\n",
    "              \"test_path\" : None,\n",
    "              # \"num_workers\" : 1, \n",
    "              # \"pin_memory\" : True,      \n",
    "              \"num_neurons\" : None,               \n",
    "              \"num_neurons2\" : None,\n",
    "              \"optimizer\" : None,\n",
    "              \"dropout\" : None,\n",
    "              \"loss\": None,\n",
    "              \"gamma\" : 0.1,\n",
    "              \"validation_loss\" : None,\n",
    "              \"training_loss\" : None,\n",
    "              \"experiment\": None}\n",
    "\n",
    "# all of the below parameters can be modified according to your preferences\n",
    "\n",
    "# batch_size = [32, 64, 128]            # same for training validation and test set, values for first part of the training\n",
    "\n",
    "batch_size = {\"fluxes\" : [32, 64, 128], \"genes\" : [32, 64, 128], \"all\" : [32, 64, 128], \n",
    "              \"RFECV_genes\" : [32, 64, 128], \"RFECV_fluxes\" : [32, 64, 128], \"RFECV_all\" : [32, 64, 128], \n",
    "              \"RFECV_genes_p\" : [32, 64, 128], \"FCBF_fluxes_p\" : [32, 64, 128], \n",
    "              \"FCBF_fluxes\" : [32, 64, 128], \"FCBF_genes\" : [32, 64, 128], \"FCBF_all\" : [32, 64, 128], \n",
    "              \"FCBF_genes_p\" : [32, 64, 128], \"FCBF_all_p\" : [32, 64, 128]}\n",
    "\n",
    "# epochs = [400, 800, 1200, 1600, 2000, 2400]      # values for first part of the training\n",
    "\n",
    "epochs = {\"fluxes\" : [400, 800, 1200, 1600, 2000, 2400], \"genes\" : [400, 800, 1200, 1600, 2000, 2400], \n",
    "          \"all\" : [400, 800, 1200, 1600, 2000, 2400], \"RFECV_genes\" : [400, 800, 1200, 1600, 2000, 2400], \n",
    "          \"RFECV_fluxes\" : [400, 800, 1200, 1600, 2000, 2400], \"RFECV_all\" : [400, 800, 1200, 1600, 2000, 2400], \n",
    "          \"RFECV_genes_p\" : [400, 800, 1200, 1600, 2000, 2400], \"FCBF_fluxes_p\" : [400, 800, 1200, 1600, 2000, 2400], \n",
    "          \"FCBF_fluxes\" : [400, 800, 1200, 1600, 2000, 2400], \"FCBF_genes\" : [400, 800, 1200, 1600, 2000, 2400], \n",
    "          \"FCBF_all\" : [400, 800, 1200, 1600, 2000, 2400], \"FCBF_genes_p\" : [400, 800, 1200, 1600, 2000, 2400], \n",
    "          \"FCBF_all_p\" : [400, 800, 1200, 1600, 2000, 2400]}\n",
    "\n",
    "# learning_rate =  [1e-2, 1e-3, 1e-4, 1e-5]        # values for first part of the training\n",
    "\n",
    "learning_rate = {\"fluxes\" : [1e-2, 1e-3, 1e-4, 1e-5], \"genes\" : [1e-2, 1e-3, 1e-4, 1e-5], \"all\" : [1e-2, 1e-3, 1e-4, 1e-5], \n",
    "                 \"RFECV_genes\" : [1e-2, 1e-3, 1e-4, 1e-5], \"RFECV_fluxes\" : [1e-2, 1e-3, 1e-4, 1e-5], \n",
    "                 \"RFECV_all\" : [1e-2, 1e-3, 1e-4, 1e-5], \"RFECV_genes_p\" : [1e-2, 1e-3, 1e-4, 1e-5], \n",
    "                 \"FCBF_fluxes_p\" : [1e-2, 1e-3, 1e-4, 1e-5], \"FCBF_fluxes\" : [1e-2, 1e-3, 1e-4, 1e-5],\n",
    "                 \"FCBF_genes\" : [1e-2, 1e-3, 1e-4, 1e-5], \"FCBF_all\" : [1e-2, 1e-3, 1e-4, 1e-5], \n",
    "                 \"FCBF_genes_p\" : [1e-2, 1e-3, 1e-4, 1e-5], \"FCBF_all_p\" : [1e-2, 1e-3, 1e-4, 1e-5]}\n",
    "\n",
    "# dropout = [0, 0.3, 0.6]                          # values for first part of the training\n",
    "\n",
    "dropout = {\"fluxes\" : [0, 0.3, 0.6], \"genes\" : [0, 0.3, 0.6], \"all\" : [0, 0.3, 0.6], \n",
    "           \"RFECV_genes\" : [0, 0.3, 0.6], \"RFECV_fluxes\" : [0, 0.3, 0.6], \"RFECV_all\" : [0, 0.3, 0.6], \n",
    "           \"RFECV_genes_p\" : [0, 0.3, 0.6], \"FCBF_fluxes_p\" : [0, 0.3, 0.6], \n",
    "           \"FCBF_fluxes\" : [0, 0.3, 0.6], \"FCBF_genes\" : [0, 0.3, 0.6], \"FCBF_all\" : [0, 0.3, 0.6], \n",
    "           \"FCBF_genes_p\" : [0, 0.3, 0.6], \"FCBF_all_p\" : [0, 0.3, 0.6]}\n",
    "\n",
    "# optimizer = [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"]  # values for first part of the training\n",
    "\n",
    "optimizer = {\"fluxes\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \"genes\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \n",
    "             \"all\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \"RFECV_genes\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \n",
    "             \"RFECV_fluxes\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \"RFECV_all\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \n",
    "             \"RFECV_genes_p\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \"FCBF_fluxes_p\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"],\n",
    "             \"FCBF_fluxes\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \"FCBF_genes\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \n",
    "             \"FCBF_all\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \"FCBF_genes_p\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"], \n",
    "             \"FCBF_all_p\" : [\"Adam\", \"SGD\",\"Rprop\", \"Adadelta\"]}\n",
    "\n",
    "# loss = [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"]     # values for first part of the training\n",
    "\n",
    "loss = {\"fluxes\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \"genes\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \n",
    "        \"all\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \"RFECV_genes\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \n",
    "        \"RFECV_fluxes\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \"RFECV_all\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \n",
    "        \"RFECV_genes_p\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \"FCBF_fluxes_p\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"],\n",
    "        \"FCBF_fluxes\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \"FCBF_genes\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \n",
    "        \"FCBF_all\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \"FCBF_genes_p\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"], \n",
    "             \"FCBF_all_p\" : [\"L1_loss\", \"MSE_loss\", \"Smooth_l1_loss\"]}\n",
    "\n",
    "\n",
    "paths = [\"paths\",  \n",
    "         None # to distinguish between the two possible situations in which there is one or two datasets (not training, validation and test together)\n",
    "        ] # add the paths to the csv containing your data according to the usage of the variable paths below (modifying the indices if necessary)\n",
    "\n",
    "no_cuda = False               # SHOULD BE FALSE                    \n",
    "seed = 1                   \n",
    "log_interval = 10       \n",
    "percent_train = 0.7  \n",
    "percent_validation = 0.2\n",
    "\n",
    "dataset_path = paths[-1] \n",
    "parameters[\"dataset_path\"] = dataset_path\n",
    "\n",
    "training_path = paths[18]    \n",
    "validation_path = paths[19]\n",
    "test_path = paths[20] \n",
    "parameters[\"training_path\"] = training_path\n",
    "parameters[\"validation_path\"] = validation_path\n",
    "parameters[\"test_path\"] = test_path\n",
    "\n",
    "input_size = 32\n",
    "\n",
    "# change the values below according to the size of your dataset, the keys define the type of data\n",
    "num_neurons = {\"fluxes\" : [900, 1200, 330, 200], \"genes\" : [10000, 15000, 3500, 2500], \"all\" : [12000, 17000, 4000, 2500], \n",
    "               \"fluxes_p\" : [220, 300, 50, 30], \"genes_p\" : [3300, 2700, 800, 650], \"all_p\" : [3300, 2800, 700, 450],\n",
    "               \"RFECV_genes\" : [3400, 4100, 1100, 890], \"RFECV_all\" : [3200, 4000, 900, 750], \"RFECV_fluxes\" : [40, 52, 10, 7], \n",
    "               \"RFECV_genes_p\" : [390, 580, 130, 80], \"FCBF_all\" : [160, 230, 45, 33], \"FCBF_genes\" : [190, 270, 56, 42], \"FCBF_fluxes\" : [28, 34, 7, 5], \n",
    "               \"FCBF_fluxes_p\" : [9, 12, 3, 2], \"FCBF_genes_p\" : [55, 70, 20, 15], \"FCBF_all_p\" : [60, 80, 28, 18]}   \n",
    "                                                    \n",
    "\n",
    "num_neurons2 = {\"fluxes\" : [1500, 1800, 180, 40], \"genes\" : [20000, 28000, 4000, 5000], \"all\" : [18000, 24000, 5000, 1500], \n",
    "                \"fluxes_p\" : [300, 400, 20, 10], \"genes_p\" : [4200, 1600, 430, 250], \"all_p\" : [4200, 1600, 380, 180],\n",
    "                \"RFECV_fluxes\" : [60, 70, 5, 3], \"RFECV_genes\" : [4590, 5460, 670, 500], \"RFECV_all\" : [4400, 5200, 570, 440], \n",
    "                \"RFECV_genes_p\" : [700, 900, 70, 30], \"FCBF_all\" : [290, 330, 20, 15], \"FCBF_genes\" : [340, 400, 34, 20], \"FCBF_fluxes\" : [48, 58, 4, 3], \n",
    "                \"FCBF_fluxes_p\" : [16, 22, 2, 1], \"FCBF_genes_p\" : [80, 110, 12, 8], \"FCBF_all_p\" : [110, 140, 15, 7]}\n",
    "\n",
    "# define the mapping\n",
    "mapping = {459 : \"fluxes\", 6170 : \"genes\", 6629 : \"all\", 1858 : \"RFECV_genes\", 19 : \"RFECV_fluxes\", 1664 : \"RFECV_all\",\n",
    "           200 : \"RFECV_genes_p\", 4 : \"FCBF_fluxes_p\", 32 : \"FCBF_genes_p\", 36 : \"FCBF_all_p\",\n",
    "           79 : \"FCBF_genes\", 12 : \"FCBF_fluxes\", 68 : \"FCBF_all\"}\n",
    "num_n = mapping[input_size]\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")    # use the GPU if available\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "if dataset_path != None:\n",
    "    partition, labels = splitTrainingValidationTestSet(dataset_path, percent_train, percent_validation)\n",
    "else:\n",
    "    partition, labels = loadTrainingValidationTestSet(training_path, validation_path, test_path)\n",
    "    \n",
    "training_set = YeastDataset(partition['training'], labels)\n",
    "validation_set = YeastDataset(partition['validation'], labels)\n",
    "        \n",
    "torch.set_printoptions(precision=9)                                 # to print more digits for the loss\n",
    "\n",
    "print(\"Device: \", device)                           # simple check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomIndexes():                                           # randomly choose the parameter\n",
    "    int1 = randint(0, len(batch_size[num_n]) - 1)    \n",
    "    int2 = randint(0, len(epochs[num_n]) - 1)\n",
    "    int3 = randint(0, len(learning_rate[num_n]) - 1)\n",
    "    int4 = randint(0, len(num_neurons[num_n]) - 1)                                  \n",
    "    int5 = randint(0, len(num_neurons2[num_n]) - 1)\n",
    "    int6 = randint(0, len(optimizer[num_n]) - 1)\n",
    "    int7 = randint(0, len(dropout[num_n]) - 1)\n",
    "    int8 = randint(0, len(loss[num_n]) - 1)                                        \n",
    "    return int1, int2, int3, int4, int5, int6, int7, int8\n",
    "\n",
    "def fillDict(d):\n",
    "    ind1, ind2, ind3, ind4, ind5, ind6, ind7, ind8 = RandomIndexes()\n",
    "    d[\"batch_size\"] = batch_size[num_n][ind1]\n",
    "    d[\"epochs\"] = epochs[num_n][ind2]\n",
    "    d[\"learning_rate\"] = learning_rate[num_n][ind3]\n",
    "    d[\"num_neurons\"] = num_neurons[num_n][ind4]                          \n",
    "    d[\"num_neurons2\"] = num_neurons2[num_n][ind5]\n",
    "    d[\"optimizer\"] = optimizer[num_n][ind6]\n",
    "    d[\"dropout\"] = dropout[num_n][ind7]\n",
    "    d[\"loss\"] = loss[num_n][ind8]\n",
    "    d = OrderedDict(sorted(d.items(), key=lambda t: t[0]))\n",
    "    return d                                                                       \n",
    "\n",
    "def checkIdentical(path, parameters):\n",
    "    combinations = pd.read_csv(path) \n",
    "    if \"Unnamed: 0\" in combinations.columns:\n",
    "        combinations.drop(\"Unnamed: 0\", axis=1, inplace=True)               # change according to the structure of your file\n",
    "    combinations = combinations.to_dict('records')\n",
    "    if parameters in combinations:\n",
    "        print(\"This combination of parameters has already been used\")\n",
    "        return (False, None)                                                       # return false if combination already used\n",
    "    print(parameters)\n",
    "    return (True, parameters)                                                              # true if combination not used yet \n",
    "\n",
    "def checkForWarmStarting(path, num1, num2):                                     # returns False if no warmstarting is possible\n",
    "    df = pd.read_csv(path)\n",
    "    array = [num2]\n",
    "    df = df.loc[(df['num_neurons'] == num1) & df['num_neurons2'].isin(array)]\n",
    "    if len(df) != 0:\n",
    "        min_val = sorted(df[\"validation_loss\"])                                # returns the one with the best validation loss\n",
    "        exp = df.loc[df['validation_loss'] == min_val[0]][\"experiment\"]\n",
    "        return exp.iloc[0]\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trialRegressor(nn.Module):\n",
    "    def __init__(self, num_neurons, num_neurons2, dropout):  \n",
    "        super(trialRegressor, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_neurons2 = num_neurons2\n",
    "        self.dropout = dropout\n",
    "            \n",
    "        self.fc1 = nn.Linear(self.input_size, self.num_neurons)\n",
    "        self.fc2 = nn.Linear(self.num_neurons, self.num_neurons2)\n",
    "        self.fc3 = nn.Linear(self.num_neurons2, 1)\n",
    "        self.dropt = nn.Dropout(self.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x.float()))\n",
    "        h1 = F.relu(self.fc2(self.dropt(h)))\n",
    "        return self.fc3(self.dropt(h1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, predict, loss):\n",
    "    if loss == \"L1_loss\":\n",
    "        return F.l1_loss(predict, real)\n",
    "    elif loss == \"MSE_loss\":\n",
    "        return F.mse_loss(predict, real)\n",
    "    else:\n",
    "        return F.smooth_l1_loss(predict, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_function(optimizer, model, lr):\n",
    "    if optimizer == \"Adam\":\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"SGD\":\n",
    "        return optim.SGD(model.parameters(), lr=lr, weight_decay=0.1)\n",
    "    elif optimizer == \"RMSprop\":\n",
    "        return optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        return optim.Adadelta(model.parameters(), lr=lr)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, lo):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for _, (data, labels) in enumerate(train_loader):\n",
    "        # print(\"Epoch: {}, Batch index: {}\".format(epoch, batch_idx))\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optmz.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(labels.float(), output, lo)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optmz.step()\n",
    "    with torch.no_grad():\n",
    "        loss_evolution.append(loss.cpu().detach().numpy())   \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(          \n",
    "        epoch, loss / len(train_loader.dataset)))\n",
    "\n",
    "    # print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "    #      epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def validate(epoch, lo):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(validation_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_function(labels.float(), output, lo)\n",
    "            valid_loss += loss.item()\n",
    "            if epoch == parameters[\"epochs\"]:\n",
    "                REC_targets.append(labels.cpu().numpy().reshape(1, -1))             # take the whole dataset\n",
    "                REC_predictions.append(output.cpu().reshape(1, -1))\n",
    "        valid_loss_evolution.append(loss.cpu().numpy()) \n",
    "    # valid_loss /= len(validation_loader.dataset)                              \n",
    "    # print('====> Validation set loss: {:.4f}'.format(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(repetitions):\n",
    "    while(True):\n",
    "        flag, parameters = checkIdentical(\"path pointing to the directory where the file containing all the previously tried combination is \\Records.csv\", fillDict(parameters))\n",
    "        if flag:                                                              # if combination never used exit\n",
    "            break\n",
    "    directory_path = \"path where to save the results\" + \"\\\\\" + str(datetime.datetime.now().\n",
    "                                                                                     strftime(\"%d_%m_%Y-%H.%M\"))\n",
    "    print(\"Repetition: \", i + 1)\n",
    "    model = trialRegressor(parameters[\"num_neurons\"], parameters[\"num_neurons2\"], parameters[\"dropout\"])\n",
    "    \n",
    "                                    # check for a possible warm starting\n",
    "    check = checkForWarmStarting(\"path \\Pre_Records.csv\", #  where the file containing all the previously tried combinations (different from the one above)  \n",
    "                                 parameters[\"num_neurons\"], parameters[\"num_neurons2\"])# is stored, if you want to initialise your network with pre-copmputed weights\n",
    "    \n",
    "    if check :\n",
    "        print(\"Warm starting from\", check)\n",
    "        model.load_state_dict(torch.load(\"path to the directory where the associated weights are stored\" + \"weights.pt\"), \n",
    "                                         strict=True)\n",
    "        \n",
    "    model.to(device)\n",
    "      \n",
    "    optmz = optimizer_function(parameters[\"optimizer\"], model, parameters[\"learning_rate\"])\n",
    "    step_size = parameters[\"epochs\"]//2\n",
    "    gamma = parameters[\"gamma\"]\n",
    "    scheduler = optim.lr_scheduler.StepLR(optmz, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(training_set, shuffle=True, batch_size=parameters[\"batch_size\"], **kwargs)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=parameters[\"batch_size\"], shuffle=True, **kwargs)\n",
    "    \n",
    "    REC_targets = list()\n",
    "    REC_predictions = list()\n",
    "    loss_evolution = list()\n",
    "    valid_loss_evolution = list()\n",
    "    \n",
    "    # training\n",
    "    for epoch in range(1, parameters[\"epochs\"] + 1):                                   \n",
    "        scheduler.step()\n",
    "        train(epoch, parameters[\"loss\"])\n",
    "        validate(epoch, parameters[\"loss\"])  \n",
    "        \n",
    "    # save the model    \n",
    "    if not os.path.exists(directory_path):                \n",
    "        os.makedirs(directory_path)\n",
    "    \n",
    "    #save the losses for comparison (superfluous given the saves in the csv file)\n",
    "    text_file = open(\"where to save the losses \\Losses.txt\", \"a\")\n",
    "    text_file.write(\"\\n\" + directory_path[66:] + \"  \" + str(valid_loss_evolution[-10:]))\n",
    "    text_file.close()\n",
    "    \n",
    "    torch.save(model.state_dict(), directory_path + \"\\\\weights.pt\")   \n",
    "    \n",
    "    visualizeLossesOverEpochs(valid_loss_evolution, loss_evolution, 0,\n",
    "                          \"Validation loss\", \"Training loss\", \"0\", 50, 100, 'o-', False)\n",
    "\n",
    "    plt.savefig(fname=directory_path + \"\\\\validation_loss.png\", bbox_inches=\"tight\")\n",
    "    \n",
    "    # save full validation loss\n",
    "    text_file = open(directory_path +\"\\\\validation_loss.txt\", \"w\")\n",
    "    text_file.write(str(valid_loss_evolution) + \"\\n\")\n",
    "    text_file.close()\n",
    "    \n",
    "    # save last batch targets\n",
    "    text_file = open(directory_path +\"\\\\REC_targets.txt\", \"w\")\n",
    "    text_file.write(str(REC_targets) + \"\\n\")\n",
    "    text_file.close()\n",
    "    \n",
    "    # save last batch predictions\n",
    "    text_file = open(directory_path +\"\\\\REC_predictions.txt\", \"w\")\n",
    "    text_file.write(str(REC_predictions) + \"\\n\")\n",
    "    text_file.close()\n",
    "    \n",
    "    # write the parameters in the csv file\n",
    "    combinations = pd.read_csv(\"path \\Records.csv\") \n",
    "    if \"Unnamed: 0\" in combinations.columns:\n",
    "        combinations.drop(\"Unnamed: 0\", axis=1, inplace=True)                # change according to the structure of your file\n",
    "    combinations = combinations.to_dict('records')\n",
    "    parameters[\"training_loss\"] = loss_evolution[-1]\n",
    "    parameters[\"validation_loss\"] = valid_loss_evolution[-1]\n",
    "    parameters[\"training_path\"] = str(training_path)\n",
    "    parameters[\"test_path\"] = str(test_path)\n",
    "    parameters[\"experiment\"] = directory_path[66:]\n",
    "    combinations.append(parameters)                                               \n",
    "    records = pd.DataFrame(combinations)\n",
    "    records.to_csv(\"path \\Records.csv\", encoding=\"utf-8\")\n",
    "    \n",
    "    text_file = open(directory_path + \"\\\\\" + \"Parameters.txt\", \"w\")\n",
    "    text_file.write(\"Parameters used: \\n\\n\")\n",
    "    for _, (key, value) in enumerate(parameters.items()):\n",
    "        text_file.write(key + \" = \" + str(value) + \"\\n\")\n",
    "    text_file.write(\"\\n Dataset path: \" + str(dataset_path))\n",
    "    text_file.write(\"\\n Training set path: \" + str(training_path))  \n",
    "    text_file.write(\"\\n Test set path: \" + str(test_path))\n",
    "    \n",
    "    text_file.close()\n",
    "    \n",
    "    plt.close(\"all\")                                            # close all the figures in order to save memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
